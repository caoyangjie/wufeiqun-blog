## 背景介绍

最近公司正在进行容器化迁移, 将部署在虚拟机上的Springboot应用迁移到阿里云ACK集群中, 迁移到容器以后, 有开发同学反馈前端调用后端接口偶尔出现`502`的现象, 之前在虚拟机的时候没有出现或者没有察觉, 现在是频率很高, 非常明显.

数据流大致如下:

```
前端/小程序  ---> Nginx ----> Nodeport(IPVS) ---> Pod
```



## 排查过程

#### 调整POD的limits大小

因为公司是toB的, 业务访问量一般非常小, 默认容器的CPU限制都是给了`1核`, 测试环境也简单压测了一下, 是可以复现的, 就是CPU给的少的时候出现了502, CPU给的多的时候没有出现, 所以统一将所有上容器的应用的CPU限制改成了`4核`, 过了一天, 开发同学反馈还是有, 那应该还有其它的原因了

#### 怀疑跟undertow的配置有关

公司有几个不使用undertow的应用, 没有出现502, 出现502的都是使用的undertow, 怀疑跟undertow的配置有关, undertow中有一个配置比较怀疑, 

```
no-request-timeout=-1
```

因为涉及到业务, 而且确定性也不是很强就没有去改这块, 继续去探索其他的可能


#### 查看Nginx的错误日志

前面排查的时候都是自己瞎猜的, 这会儿没有思路了, 想起来了要查看Nginx的异常日志了, 发现每次给前端返回502的时候Nginx总是报错如下:

```
2023/05/27 00:20:40 [error] 236823#236823: *439448513 recv() failed (104: Connection reset by peer) while reading response header from upstream
```

后来从网上查了一下, 大概的意思是:

```
服务端确实已经关闭了连接： upstream发送了RST，将连接重置。

当upstream后端的server先于Nginx断开连接的时候, Nginx依然向后端发送请求, 这时候就会出现该报错.
```

Nginx我们这边使用的是`1.10.1`版本, upstream中的keepalive是没有配置超时时间的, 也就是如果底层操作系统支持, 它会一直跟后端连接着, 再看看上面说的服务端的undertow的参数, 也是永久连接的, 不会主动跟调用方断开,  那就看看操作系统和ipvs的超时配置吧

Nginx的TCP keepalive超时配置如下:


以下表示该连接30秒空闲的时候就开启检查了, 检查间隔75秒, 总共检查9次, 也就是将近11分钟后才会断开

```
net.ipv4.tcp_keepalive_time = 30
net.ipv4.tcp_keepalive_probes = 9
net.ipv4.tcp_keepalive_intvl = 75
```


k8s Node节点的配置:

```
net.ipv4.tcp_keepalive_intvl = 75
net.ipv4.tcp_keepalive_probes = 9
net.ipv4.tcp_keepalive_time = 7200
```

k8s IPVS超时时间:

也就是900秒, 15分钟

```
# ipvsadm -l --timeout
Timeout (tcp tcpfin udp): 900 120 300
```

发现Nginx超时时间是比上游服务器的短的, 貌似也说不通, 暂时先把Nginx的upstream改成短连接了, 也就是去掉了`keepalive_time`参数, 确实是管用了


## 参考链接

* https://www.cnblogs.com/lizexiong/p/15358894.html
* https://stephenzhou.net/2018/11/13/nginx-502/
* https://man7.org/linux/man-pages/man7/tcp.7.html
* https://qichehuizhan.com/2022/10/16/Wiki/1.SRE/2.K8S%E7%BB%B4%E5%BA%A6/6.Kubernetes_IPVS%E7%9B%B8%E5%85%B3%E9%97%AE%E7%AD%94%E9%A2%98%E8%AE%B0%E5%BD%95/
* https://blog.csdn.net/Junzizhiai/article/details/113862678
* https://github.com/cloudnativelabs/kube-router/issues/521
* https://blog.frognew.com/2018/12/kubernetes-ipvs-long-connection-optimize.html

